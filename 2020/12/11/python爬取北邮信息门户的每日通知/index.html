<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="python爬取北邮信息门户的每日通知"><meta name="keywords" content=""><meta name="author" content="byrwyj"><meta name="copyright" content="byrwyj"><title>python爬取北邮信息门户的每日通知 | byrwyj的博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="转到本文目录">查看所有文章</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、导入相关库"><span class="toc-number">1.1.</span> <span class="toc-text">一、导入相关库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、模拟登录"><span class="toc-number">1.2.</span> <span class="toc-text">二、模拟登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、爬取信息"><span class="toc-number">1.3.</span> <span class="toc-text">二、爬取信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四、储存数据"><span class="toc-number">1.4.</span> <span class="toc-text">四、储存数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五、整体代码"><span class="toc-number">1.5.</span> <span class="toc-text">五、整体代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#六、这期间的一些坑"><span class="toc-number">1.6.</span> <span class="toc-text">六、这期间的一些坑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#七、结束语"><span class="toc-number">1.7.</span> <span class="toc-text">七、结束语</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://s1.ax1x.com/2020/06/26/NDLim4.jpg"></div><div class="author-info__name text-center">byrwyj</div><div class="author-info__description text-center">北邮</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">byrwyj的博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/archives">时间轴</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">python爬取北邮信息门户的每日通知</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-11</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>python3.7爬虫初学者实例、北邮信息门户、爬取每日通知并下载相关文件一齐保存到桌面</p>
<a id="more"></a>

<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对这篇博客的帮助很大的学习资料：<br>1.<a href="https://study.163.com/course/introduction.htm?courseId=1003285002&_trace_c_p_k2_=03d854533f88454dbc3e08123edb68e7" target="_blank" rel="noopener">网易云课堂Python网络爬虫实战</a>里面的视频很有用，建议认真学一下。</p>
<p>2.博主kelvinmao的博客<a href="https://blog.csdn.net/kelvinmao/article/details/51628649" target="_blank" rel="noopener">python网络爬虫学习(五) 模拟登陆北邮信息门户并爬取信息</a>.让我减少了登陆验证的许多繁琐的事情，但也不知对我的能力提升是好是坏?</p>
<h2 id="一、导入相关库"><a href="#一、导入相关库" class="headerlink" title="一、导入相关库"></a>一、导入相关库</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">today = datetime.date.today().isoformat()<span class="comment">#日期格式 2019-07-05</span></span><br><span class="line"><span class="comment">#在桌面建立一个文件夹用于储存文件</span></span><br><span class="line">folder_path = <span class="string">'C:/Users/john/OneDrive/桌面/'</span> + today +<span class="string">"/"</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder_path):</span><br><span class="line">    os.makedirs(folder_path)</span><br></pre></td></tr></table></figure>

<h2 id="二、模拟登录"><a href="#二、模拟登录" class="headerlink" title="二、模拟登录"></a>二、模拟登录</h2><p><img src="https://img-blog.csdnimg.cn/20190705193201445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg4NDk1NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>分析提交的表单</strong><img src="https://img-blog.csdnimg.cn/2019070518431094.png" alt=""><br>1.username#学号<br>2.password#密码<br>3.lt #这个是webflow发放的流水号<br>4.execution#细心即可发现，是一个不变的值<br>5._eventId:#也是一个不变的值<br>6.rmShown#同是一个不变的值<br>ps：关于lt：按我的理解解释一下：打开网页（即GET请求）时，会有一个流水号，我们可以在网页源代码中找到它。现在又出现一个问题，第二次POST请求（携带表单数据）时，lt就会变化，如何解决？答：可以使用requests的session方法来保持cookie，lt等参数不变。（相当于还是第一次的请求，不过是携带了数据）<br><img src="https://img-blog.csdnimg.cn/20190705190837962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg4NDk1NQ==,size_16,color_FFFFFF,t_70" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模拟一个浏览器头</span></span><br><span class="line">header=&#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:47.0) Gecko/20100101 Firefox/47.0'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#setting cookie</span></span><br><span class="line">s=requests.Session()</span><br><span class="line">s.cookies=cookielib.CookieJar()</span><br><span class="line">r=s.get(<span class="string">'https://auth.bupt.edu.cn/authserver/login?service=http%3A%2F%2Fmy.bupt.edu.cn%2Findex.portal'</span>,headers=header)</span><br><span class="line">dic=getLt(r.text)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLt</span><span class="params">(str)</span>:</span></span><br><span class="line">    lt=bs(str,<span class="string">'html.parser'</span>)</span><br><span class="line">    dic=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> inp <span class="keyword">in</span> lt.form.find_all(<span class="string">'input'</span>):</span><br><span class="line">        <span class="keyword">if</span>(inp.get(<span class="string">'name'</span>))!=<span class="literal">None</span>:</span><br><span class="line">            dic[inp.get(<span class="string">'name'</span>)]=inp.get(<span class="string">'value'</span>)</span><br><span class="line">    print(dic)</span><br><span class="line">    <span class="keyword">return</span> dic</span><br><span class="line"></span><br><span class="line">postdata=&#123;</span><br><span class="line">    <span class="string">'username'</span>:<span class="string">'######'</span>,<span class="comment">#此处为你的学号</span></span><br><span class="line">    <span class="string">'password'</span>:<span class="string">'######'</span>,<span class="comment">#你的密码</span></span><br><span class="line">    <span class="string">'lt'</span>:dic[<span class="string">'lt'</span>],</span><br><span class="line">    <span class="string">'execution'</span>:<span class="string">'e1s1'</span>,</span><br><span class="line">    <span class="string">'_eventId'</span>:<span class="string">'submit'</span>,</span><br><span class="line">    <span class="string">'rmShown'</span>:<span class="string">'1'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="二、爬取信息"><a href="#二、爬取信息" class="headerlink" title="二、爬取信息"></a>二、爬取信息</h2><p>1.登录</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#携带登陆数据，以post方式登录，</span></span><br><span class="line">response=s.post(<span class="string">'https://auth.bupt.edu.cn/authserver/login?service=http%3A%2F%2Fmy.bupt.edu.cn%2Findex.portal'</span>,data=postdata,headers=header)</span><br><span class="line"><span class="comment">#用get方式访问“校内通知”的页面</span></span><br><span class="line">res=s.get(<span class="string">'http://my.bupt.edu.cn/index.portal?.pn=p1778'</span>,headers=header)</span><br><span class="line"><span class="comment">#用beautifulsoup解析html</span></span><br><span class="line">soup=bs(res.text,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure>
<p>2.查找目标url<br><img src="https://img-blog.csdnimg.cn/2019070519360847.png" alt="在这里插入图片描述"><br>如上图，a标签的href加上前缀就是超链接目标的URL，是我需要的信息，但是发现这个a标签既没有id，也没有class，所以使用re.compile（）函数，发现每个href的前半部分都是一样的,故使字符串“detach”来进行匹配</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"detach"</span>)):</span><br><span class="line">    url.append(<span class="string">'http://my.bupt.edu.cn/'</span>+j.get(<span class="string">'href'</span>))</span><br></pre></td></tr></table></figure>
<p>3.查找通知发布的日期。<br><img src="https://img-blog.csdnimg.cn/20190705194810143.png" alt="在这里插入图片描述"><br>日期的class为time，故代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">date=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> soup.find_all(class_=<span class="string">'time'</span>):</span><br><span class="line">    date.append(j.string)</span><br></pre></td></tr></table></figure>
<p><strong>4.爬取内文信息</strong><br>①标题的class=’.text-center’，使用soup.select(）函数<img src="https://img-blog.csdnimg.cn/20190705195037995.png" alt="在这里插入图片描述"></p>
<p>②具体内容的class=’singleinfo’，里面的全部的p标签的内容需要合并<br><img src="https://img-blog.csdnimg.cn/20190705195254318.png" alt=""><br>③如果有文件，需要下载下来，文件的url处理方法和二、2一样，采用re.compile（）函数；下载时使用open（），参数‘wb’（以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。）<br><img src="https://img-blog.csdnimg.cn/20190705195732599.png" alt="在这里插入图片描述"><br><strong>此函数功能：传入url，下载文件，返回一个dict，包含title和article。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNewsDetail</span><span class="params">(newsurl)</span>:</span></span><br><span class="line">    result=&#123;&#125;</span><br><span class="line">    res=s.get(newsurl,headers=header)</span><br><span class="line">    res.encoding=<span class="string">'utf-8'</span></span><br><span class="line">    soup=bs(res.text,<span class="string">'html.parser'</span>)</span><br><span class="line">    result[<span class="string">'Title'</span>]=soup.select(<span class="string">'.text-center'</span>)[<span class="number">0</span>].text</span><br><span class="line">    article=[]</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> soup.select(<span class="string">'.singleinfo p'</span>):</span><br><span class="line">       article.append(p.text.strip())</span><br><span class="line">    result[<span class="string">'article'</span>]=article[<span class="number">0</span>]</span><br><span class="line">    downloadurl=[]</span><br><span class="line">    filename=[]</span><br><span class="line">    Docurl=soup.find_all(href=re.compile(<span class="string">"attachment"</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> Docurl:</span><br><span class="line">        downloadurl.append(<span class="string">'http://my.bupt.edu.cn/'</span>+k.get(<span class="string">'href'</span>))</span><br><span class="line">        filename.append(k.string)</span><br><span class="line">    <span class="keyword">if</span>  filename:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,len(filename)):</span><br><span class="line">            download=s.get(downloadurl[k],headers=header)</span><br><span class="line">            <span class="keyword">with</span> open(folder_path+filename[k],<span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(download.content)</span><br><span class="line">            f.close()</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h2 id="四、储存数据"><a href="#四、储存数据" class="headerlink" title="四、储存数据"></a>四、储存数据</h2><p>如果日期为今天，就调用函数getNewsDetail(）然后存入news_totol[],最后使用pandas的dataframe（）和to_excel（），保存文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">news_total=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">29</span>):</span><br><span class="line">    <span class="keyword">if</span> date[i]!=today+<span class="string">' '</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    newsary=getNewsDetail(url[i])</span><br><span class="line">    news_total.append(newsary)</span><br><span class="line">df=pd.DataFrame(news_total)</span><br><span class="line">df.to_excel(folder_path+<span class="string">'news.xlsx'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="五、整体代码"><a href="#五、整体代码" class="headerlink" title="五、整体代码"></a>五、整体代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Fri Jul  5 16:49:28 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: byrwyj</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">today = datetime.date.today().isoformat()</span><br><span class="line">folder_path = <span class="string">'C:/Users/john/OneDrive/桌面/'</span> + today +<span class="string">"/"</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder_path):</span><br><span class="line">    os.makedirs(folder_path)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLt</span><span class="params">(str)</span>:</span></span><br><span class="line">    lt=bs(str,<span class="string">'html.parser'</span>)</span><br><span class="line">    dic=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> inp <span class="keyword">in</span> lt.form.find_all(<span class="string">'input'</span>):</span><br><span class="line">        <span class="keyword">if</span>(inp.get(<span class="string">'name'</span>))!=<span class="literal">None</span>:</span><br><span class="line">            dic[inp.get(<span class="string">'name'</span>)]=inp.get(<span class="string">'value'</span>)</span><br><span class="line">    <span class="keyword">return</span> dic</span><br><span class="line"></span><br><span class="line">header=&#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#setting cookie</span></span><br><span class="line">s=requests.Session()</span><br><span class="line">s.cookies=cookielib.CookieJar()</span><br><span class="line">r=s.get(<span class="string">'https://auth.bupt.edu.cn/authserver/login?service=http%3A%2F%2Fmy.bupt.edu.cn%2Findex.portal'</span>,headers=header)</span><br><span class="line">dic=getLt(r.text)</span><br><span class="line">postdata=&#123;</span><br><span class="line">    <span class="string">'username'</span>:<span class="string">'######'</span>,<span class="comment">#此处为你的学号</span></span><br><span class="line">    <span class="string">'password'</span>:<span class="string">'######'</span>,<span class="comment">#你的密码</span></span><br><span class="line">    <span class="string">'lt'</span>:dic[<span class="string">'lt'</span>],</span><br><span class="line">    <span class="string">'execution'</span>:<span class="string">'e1s1'</span>,</span><br><span class="line">    <span class="string">'_eventId'</span>:<span class="string">'submit'</span>,</span><br><span class="line">    <span class="string">'rmShown'</span>:<span class="string">'1'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNewsDetail</span><span class="params">(newsurl)</span>:</span></span><br><span class="line">    result=&#123;&#125;</span><br><span class="line">    res=s.get(newsurl,headers=header)</span><br><span class="line">    res.encoding=<span class="string">'utf-8'</span></span><br><span class="line">    soup=bs(res.text,<span class="string">'html.parser'</span>)</span><br><span class="line">    result[<span class="string">'Title'</span>]=soup.select(<span class="string">'.text-center'</span>)[<span class="number">0</span>].text</span><br><span class="line">    article=[]</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> soup.select(<span class="string">'.singleinfo p'</span>):</span><br><span class="line">       article.append(p.text.strip())</span><br><span class="line">    result[<span class="string">'article'</span>]=article[<span class="number">0</span>]</span><br><span class="line">    downloadurl=[]</span><br><span class="line">    filename=[]</span><br><span class="line">    Docurl=soup.find_all(href=re.compile(<span class="string">"attachment"</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> Docurl:</span><br><span class="line">        downloadurl.append(<span class="string">'http://my.bupt.edu.cn/'</span>+k.get(<span class="string">'href'</span>))</span><br><span class="line">        filename.append(k.string)</span><br><span class="line">    <span class="keyword">if</span>  filename:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,len(filename)):</span><br><span class="line">            download=s.get(downloadurl[k],headers=header)</span><br><span class="line">            <span class="keyword">with</span> open(folder_path+filename[k],<span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(download.content)</span><br><span class="line">            f.close()</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">response=s.post(<span class="string">'https://auth.bupt.edu.cn/authserver/login?service=http%3A%2F%2Fmy.bupt.edu.cn%2Findex.portal'</span>,data=postdata,headers=header)</span><br><span class="line">res=s.get(<span class="string">'http://my.bupt.edu.cn/index.portal?.pn=p1778'</span>,headers=header)</span><br><span class="line">soup=bs(res.text,<span class="string">'html.parser'</span>)</span><br><span class="line">news_total=[]</span><br><span class="line">date=[]</span><br><span class="line">url=[]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"detach"</span>)):</span><br><span class="line">    url.append(<span class="string">'http://my.bupt.edu.cn/'</span>+j.get(<span class="string">'href'</span>))</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> soup.find_all(class_=<span class="string">'time'</span>):</span><br><span class="line">    date.append(j.string)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">29</span>):</span><br><span class="line">    <span class="keyword">if</span> date[i]!=today+<span class="string">' '</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    newsary=getNewsDetail(url[i])</span><br><span class="line">    news_total.append(newsary)</span><br><span class="line">df=pd.DataFrame(news_total)</span><br><span class="line">df.to_excel(folder_path+<span class="string">'news.xlsx'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="六、这期间的一些坑"><a href="#六、这期间的一些坑" class="headerlink" title="六、这期间的一些坑"></a>六、这期间的一些坑</h2><p>1.看教学视频使用2倍速，还不听声音，结果大的框架没学会，又得回去看?<br>2.被正则表达式弄得头晕，后来发现又可以不用它，正则表达式真是得细心学，一点一点写，不能急。<br>3.到最后的时候被文件路径的/还是\给弄晕了，导致文件怎么也删除不掉，提示路径错误。重启、压缩后删除、文件粉碎器都试过了，仍是不行。这狗皮膏药真是烦人，最终还是被解决了?。①.打开记事本输入<br>DEL /F /A /Q \?%1<br>RD /S /Q \?%1<br>②.把记事本另存为del.bat<br>③.把要删除文件用鼠标拖入del.bat，删除成功！<img src="https://img-blog.csdnimg.cn/20190705201810572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzg4NDk1NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>DEL /F /A /Q \?%1<br>RD /S /Q \?%1<br>全句意思是：强制删除系统文件夹下所有的格式为tmp的文件(哪怕文件是只读的)，并且在删除时不用向用户询问是否继续或终止!<br><strong>del 删除命令。</strong><br>/F            强制删除只读文件。<br>/S            从所有子目录删除指定文件。<br>/Q            安静模式。删除全局通配符时，不要求确认。<br>/A            根据属性选择要删除的文件。<br>/S 除目录本身外，还将删除指定目录下的所有子目录和<br>文件。用于删除目录树。<br>/Q 安静模式，加 /S 时，删除目录树结构不再要求确认<br>\?%1 表示是此文件自己</p>
<h2 id="七、结束语"><a href="#七、结束语" class="headerlink" title="七、结束语"></a>七、结束语</h2><p>这篇代码肯定有许多不完备的地方，但我写的少，也改进不了什么。还是自己太菜?。闲暇的时间才是自己的，才能做一些事情，这篇文章花了我两天时间，挺好，希望自己能学到更多的知识。</p>
<p>2019年07月05日</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">byrwyj</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://byrwyj.github.io/archives/2020/12/11/python%E7%88%AC%E5%8F%96%E5%8C%97%E9%82%AE%E4%BF%A1%E6%81%AF%E9%97%A8%E6%88%B7%E7%9A%84%E6%AF%8F%E6%97%A5%E9%80%9A%E7%9F%A5/">http://byrwyj.github.io/archives/2020/12/11/python%E7%88%AC%E5%8F%96%E5%8C%97%E9%82%AE%E4%BF%A1%E6%81%AF%E9%97%A8%E6%88%B7%E7%9A%84%E6%AF%8F%E6%97%A5%E9%80%9A%E7%9F%A5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://byrwyj.github.io/archives">byrwyj的博客</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="next-post pull-right"><a href="/2020/11/29/%E6%9E%84%E5%BB%BA%E4%BA%BA%E7%B1%BB%E5%91%BD%E8%BF%90%E5%85%B1%E5%90%8C%E4%BD%93%EF%BC%8C%E4%B8%AD%E5%9B%BD%E9%87%87%E5%8F%96%E4%BA%86%E5%93%AA%E4%BA%9B%E8%A1%8C%E5%8A%A8%EF%BC%9F/"><span>构建人类命运共同体，中国采取了哪些行动？</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'cb7e37cc2168817a9ea9',
  clientSecret: 'ba9f8372879055b454150c6d13dffa3ad7e22da4',
  repo: 'byrwyj.github.io',
  owner: 'byrwyj',
  admin: 'byrwyj',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2020 By byrwyj</div><div class="framework-info"><span>框架 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>